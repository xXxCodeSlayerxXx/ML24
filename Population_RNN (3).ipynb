{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hh8HxItIhUmj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\William\\anaconda3\\envs\\ML24\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from keras.src.optimizers import Adam, SGD, RMSprop, Adagrad, Adadelta\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit, GridSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import warnings\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "from tuning import hyperparameter_tuning\n",
    "\n",
    "param_grid = {\n",
    "    'look_back': [1,2,3],\n",
    "    'epochs': [10, 32],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'neurons': [32, 64, 128, 256],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "np.random.seed(84)\n",
    "tf.random.set_seed(84)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(84)\n",
    "tf.random.set_seed(84)\n",
    "\n",
    "scaled_cols = ['GDP (current LCU) [NY.GDP.MKTP.CN]', 'Current health expenditure per capita (current US$) [SH.XPD.CHEX.PC.CD]',\n",
    "               'Life expectancy at birth, total (years) [SP.DYN.LE00.IN]',\n",
    "               'Urban population (% of total population) [SP.URB.TOTL.IN.ZS]',\n",
    "               'Political Stability and Absence of Violence/Terrorism: Estimate [PV.EST]', 'Government Effectiveness: Estimate [GE.EST]',\n",
    "               'Control of Corruption: Estimate [CC.EST]', 'Population density [EN.POP.DNST]']\n",
    "\n",
    "num_features = len(scaled_cols)\n",
    "\n",
    "def create_dataset(dataset, look_back = 2):\n",
    "    dataX, dataY = [], []\n",
    "    dataset = dataset.reset_index(drop=True)\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        a = dataset.loc[i:(i + look_back), scaled_cols].values  # Ends at `look_back - 1` to get 5 steps\n",
    "        dataX.append(a)\n",
    "        # Assumes the target is right after the last step in `a`\n",
    "        dataY.append(dataset.loc[i + look_back, 'Population growth (annual %) [SP.POP.GROW]'])\n",
    "\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def split_data(data, cols_to_scale, test_length):\n",
    "    years = data['Time'].unique()\n",
    "\n",
    "    # Adjusting the split based on sample_length\n",
    "    train_years = years[:-test_length]\n",
    "    test_years1 = years[-test_length:int(-test_length/2)]\n",
    "    test_years2 = years[int(-test_length/2):]\n",
    "\n",
    "    train_data = data[data['Time'].isin(train_years)]\n",
    "    test_data1 = data[data['Time'].isin(test_years1)]\n",
    "    test_data2 = data[data['Time'].isin(test_years2)]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    # Fitting the scaler only on the training data\n",
    "    scaler.fit(train_data[scaled_cols])\n",
    "\n",
    "    # Transforming both training and testing data\n",
    "    # train_data.loc[:, scaled_cols] = scaler.transform(train_data[scaled_cols])\n",
    "    test_data1.loc[:, scaled_cols] = scaler.transform(test_data1[scaled_cols])\n",
    "    test_data2.loc[:, scaled_cols] = scaler.transform(test_data2[scaled_cols])\n",
    "\n",
    "    print(\"[INFO] Data successfully split into training and testing datasets.\")\n",
    "    # train_data.to_csv(\"train_scaled.csv\", index=False)\n",
    "    test_data1.to_csv(\"test1_scaled.csv\", index=False)\n",
    "    test_data2.to_csv(\"test1_scaled.csv\", index=False)\n",
    "\n",
    "    return train_data, test_data1, test_data2\n",
    "\n",
    "\n",
    "def build_model(learning_rate, num_time_steps, num_features, dropout_rate=0.2, optimizer='adam', neurons=32):\n",
    "\n",
    "    # Select optimizer\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adagrad':\n",
    "        opt = Adagrad(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adadelta':\n",
    "        opt = Adadelta(learning_rate=learning_rate)\n",
    "    else:\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model = Sequential([\n",
    "        LSTM(neurons, input_shape=(num_time_steps, num_features)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(64, activation='tanh'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=opt, loss='mse')\n",
    "    return model\n",
    "\n",
    "\n",
    "# def hyperparameter_tuning(params):\n",
    "#     # best_loss = np.inf\n",
    "#     # best_params = None\n",
    "#     # best_model = None\n",
    "#     total_loss = []\n",
    "#     n_splits = 3\n",
    "#     train_data = train_scaled\n",
    "#     look_back, epochs, learning_rate, optimizer, neurons, dropout_rate = params\n",
    "\n",
    "#     # #Cross-validation\n",
    "#     for split in range(-n_splits,-1):\n",
    "#       years = train_data['Time'].unique()\n",
    "\n",
    "#       #Last year is for validation\n",
    "#       train_years = years[:split-1]\n",
    "#       val_years = years[split-look_back:split+1]\n",
    "\n",
    "#       model = build_model(learning_rate=learning_rate, num_time_steps=look_back,\n",
    "#                                                   num_features=len(scaled_cols), dropout_rate=dropout_rate,\n",
    "#                                                   optimizer=optimizer, neurons=neurons)\n",
    "\n",
    "#       model = fit_model(model, train_data[train_data[\"Time\"].isin(train_years)], look_back=look_back, epochs=epochs, split=\"train\")[0]\n",
    "#       val_loss = fit_model(model, train_data[train_data[\"Time\"].isin(val_years)], look_back=look_back, epochs=epochs, split=\"val\")[3]\n",
    "#       total_loss.append(val_loss)\n",
    "\n",
    "#       print('One iteration', flush=True)\n",
    "\n",
    "#     result = pd.DataFrame([{'look_back': look_back, 'epochs': epochs,\n",
    "#                             'learning_rate': learning_rate, 'optimizer': optimizer,\n",
    "#                             'neurons': neurons, 'dropout_rate': dropout_rate,\n",
    "#                             'validation_loss': np.mean(total_loss)}])\n",
    "#     return result\n",
    "\n",
    "    #   for look_back in param_grid['look_back']:\n",
    "\n",
    "    #       val_years = years[split-look_back:split+1] #Take two years from train years to make the prediction\n",
    "\n",
    "    #       for epochs in param_grid['epochs']:\n",
    "    #           for learning_rate in param_grid['learning_rate']:\n",
    "    #               for optimizer in param_grid['optimizer']:\n",
    "    #                   for neurons in param_grid['neurons']:\n",
    "    #                       for dropout_rate in param_grid['dropout_rate']:\n",
    "    #                           print(f\"Training with: look_back={look_back}, epochs={epochs}, learning_rate={learning_rate}, optimizer={optimizer}, neurons={neurons}, dropout_rate={dropout_rate}\")\n",
    "    #                           model = build_model(learning_rate=learning_rate, num_time_steps=look_back,\n",
    "    #                                               num_features=len(scaled_cols), dropout_rate=dropout_rate,\n",
    "    #                                               optimizer=optimizer, neurons=neurons)\n",
    "\n",
    "    #                           model = fit_model(model, train_data[train_data[\"Time\"].isin(train_years)], look_back=look_back, epochs=epochs, split=\"train\")[0]\n",
    "    #                           val_loss = fit_model(model, train_data[train_data[\"Time\"].isin(val_years)], look_back=look_back, epochs=epochs, split=\"val\")[3]\n",
    "\n",
    "    #                           result = pd.DataFrame([{'val_year' : years[split], 'look_back': look_back, 'epochs': epochs,\n",
    "    #                                                   'learning_rate': learning_rate, 'optimizer': optimizer,\n",
    "    #                                                   'neurons': neurons, 'dropout_rate': dropout_rate,\n",
    "    #                                                   'validation_loss': val_loss}])\n",
    "\n",
    "    #                           result.to_csv(\"results_file.csv\", mode='a', header=False, index=False)\n",
    "\n",
    "    #                           if val_loss < best_loss:\n",
    "    #                               best_loss = val_loss\n",
    "    #                               best_params = {\n",
    "    #                                   'look_back': look_back,\n",
    "    #                                   'epochs': epochs,\n",
    "    #                                   'learning_rate': learning_rate,\n",
    "    #                                   'optimizer': optimizer,\n",
    "    #                                   'neurons': neurons,\n",
    "    #                                   'dropout_rate': dropout_rate\n",
    "    #                               }\n",
    "    #                               best_model = model\n",
    "\n",
    "    #                           print(f\"Trial completed with validation loss={val_loss}\")\n",
    "\n",
    "    #   print(f\"Best parameters: {best_params} with loss={best_loss}\")\n",
    "    # return best_params, best_model\n",
    "\n",
    "\n",
    "# def fit_model(model, data, look_back = 2, epochs=1, split=\"train\"):\n",
    "#     FourD_dataX, FourD_dataY = [], []\n",
    "#     countries = data['Country Name'].unique()\n",
    "#     np.random.shuffle(countries)  # Shuffle countries at each epoch\n",
    "#     total_loss = 0\n",
    "#     losses = {}\n",
    "#     all_actuals = []\n",
    "#     all_predictions = []\n",
    "#     for country in countries:\n",
    "#       X_country, y_country = create_dataset(data[data['Country Name'] == country].reset_index(drop=True),\n",
    "#                                             look_back)\n",
    "#       FourD_dataX.append(X_country)\n",
    "#       FourD_dataY.append(y_country)\n",
    "\n",
    "#       if len(X_country) == 0:\n",
    "#           print(f\"Skipping country {country} due to insufficient data.\")\n",
    "#           continue\n",
    "#       if split == \"train\":\n",
    "#         model.fit(X_country, y_country, epochs=epochs, verbose=0)\n",
    "#       if split == \"val\" or split == \"test\":\n",
    "#         country_predict = model.predict(X_country, verbose=0)\n",
    "#         total_loss += (y_country - country_predict)**2\n",
    "#         if split == \"test\":\n",
    "#           losses[country] = (y_country - country_predict)**2\n",
    "#           all_actuals.extend(y_country)\n",
    "#           all_predictions.extend(country_predict.flatten())\n",
    "\n",
    "#     stacked_dataX = np.stack(np.array(FourD_dataX), axis=0)\n",
    "#     stacked_dataY = np.stack(np.array(FourD_dataY), axis=0)\n",
    "\n",
    "#     total_loss = total_loss/len(countries)\n",
    "\n",
    "#     return model, stacked_dataX, stacked_dataY, total_loss, losses, all_actuals, all_predictions\n",
    "\n",
    "def evaluate_model(model, test_data, look_back):\n",
    "\n",
    "    avg_loss, losses, all_actuals, all_predictions = fit_model(model = model, data = test_data, look_back = look_back, epochs=1, split = \"test\")[3:]\n",
    "\n",
    "    # adding title and labels\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(all_actuals, label='Actual', color='blue')\n",
    "    plt.plot(all_predictions, label='Predicted', color='red', linestyle='--')\n",
    "    plt.title('Actual vs. Predicted Population Growth')\n",
    "    plt.xlabel('Time Step Across All Countries')\n",
    "    plt.ylabel('Population Growth (annual %)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    losses_df = pd.DataFrame(list(losses.items()), columns=['Country', 'Loss'])\n",
    "\n",
    "    # Sort DataFrame by 'Loss' in descending order and take the first 30 rows\n",
    "    losses_df = losses_df.sort_values('Loss', ascending=False).head(30)\n",
    "\n",
    "    losses_df.plot.bar(x='Country', y='Loss', figsize=(10, 6))\n",
    "    plt.title('Test Loss for Each Country')\n",
    "    plt.xlabel('Country')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the bar chart for the lowest 30 losses\n",
    "    smallest_losses_df = losses_df.sort_values('Loss').head(30)\n",
    "    smallest_losses_df.plot.bar(x='Country', y='Loss', figsize=(10, 6))\n",
    "    plt.title('Smallest 30 Losses for Each Country')\n",
    "    plt.xlabel('Country')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "    min_test_loss = losses_df['Loss'].min()\n",
    "    max_test_loss = losses_df['Loss'].max()\n",
    "    losses_df.to_csv(\"losses.csv\", index=False)\n",
    "    print(\"Average test loss: \", avg_loss)\n",
    "    print(\"Minimum test loss: \", min_test_loss)\n",
    "    print(\"Maximum test loss: \", max_test_loss)\n",
    "    print(f\"[INFO] Model evaluated successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "C_SVy1KyD8Vi",
    "outputId": "6729135b-6bd8-42d8-9354-e1da830aeafa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Data successfully split into training and testing datasets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██                                                                             | 11/432 [09:51<4:31:50, 38.74s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from multiprocess import Pool\n",
    "np.random.seed(84)\n",
    "tf.random.set_seed(84)\n",
    "\n",
    "# Load and prepare data\n",
    "df_filled = pd.read_csv('df_filled.csv')\n",
    "\n",
    "# Split data\n",
    "# train_scaled, test_scaled1, test_scaled2 = split_data(data=df_filled, cols_to_scale=scaled_cols, test_length=int(len(df_filled[\"Time\"].unique())//5))\n",
    "train, test_scaled1, test_scaled2 = split_data(data=df_filled, cols_to_scale=scaled_cols, test_length=int(len(df_filled[\"Time\"].unique())//5))\n",
    "\n",
    "params_list = [(look_back, epochs, learning_rate, optimizer, neurons, dropout_rate, train) for look_back in param_grid['look_back'] for epochs in param_grid['epochs']\n",
    "                for learning_rate in param_grid['learning_rate'] for optimizer in param_grid['optimizer'] for neurons in param_grid['neurons']\n",
    "                for dropout_rate in param_grid['dropout_rate']]\n",
    "\n",
    "\n",
    "with Pool(15) as p:\n",
    "    pool_outputs = list(\n",
    "        tqdm(\n",
    "            p.imap(hyperparameter_tuning, params_list),\n",
    "            total=len(params_list)\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(pool_outputs)\n",
    "new_dict = dict(pool_outputs)\n",
    "print(\"dict:\", new_dict)\n",
    "\n",
    "# params = hyperparameter_tuning(train_scaled, param_grid)\n",
    "# model_tuned = build_model(look_back=params[\"look_back\"], epochs=params[\"epochs\"], dropout_rate=params[\"dropout_rate\"],\n",
    "#                          optimizer=params[\"optimizer\"], neurons = params[\"neurons\"], learning_rate = params[\"learning_rate\"])\n",
    "# fitted_model = fit_model(model=model_tuned, data=train_scaled, look_back=params[\"look_back\"], epochs=params[\"epochs\"], split=\"train\")[0]\n",
    "# test_scaled1 = pd.concat(train_scaled[train_scaled[\"Time\"].isin(train_scaled[\"Time\"][-params[\"look_back\"]+1:])],test_scaled1) #add look_back years from train data to test data 1\n",
    "# test_scaled2 = pd.concat(test_scaled1[test_scaled1[\"Time\"].isin(test_scaled1[\"Time\"][-params[\"look_back\"]+1:])],test_scaled2) #add look_back years from train data to test data 1\n",
    "# evaluate_model(model=fitted_model, test_data=test_scaled1, look_back=params[\"look_back\"])\n",
    "# evaluate_model(model=fitted_model, test_data=test_scaled2, look_back=params[\"look_back\"])\n",
    "# fitted_model = fit_model(model=fitted_model, data=test_scaled1, look_back=params[\"look_back\"], epochs=params[\"epochs\"], split=\"train\")[0]\n",
    "# evaluate_model(model=fitted_model, test_data=test_scaled2, look_back=params[\"look_back\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ML24",
   "language": "python",
   "name": "ml24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
